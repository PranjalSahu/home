<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html class="gr__people_eecs_berkeley_edu"><head><meta http-equiv="Content-Type" content="text/html; charset=windows-1252">
  <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
  <style type="text/css">
  /* Design Credits: Jon Barron and Abhishek Kar and Saurabh Gupta*/
  a {
  color: #1772d0;
  text-decoration:none;
  }
  a:focus, a:hover {
  color: #f09228;
  text-decoration:none;
  }
  body,td,th {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 15px;
    font-weight: 400
  }
  heading {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 18px;
    font-weight: 1000
  }
  strong {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 15px;
    font-weight: 800
  }
  strongred {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    color: 'red' ;
    font-size: 14px
  }
  sectionheading {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 22px;
    font-weight: 600
  }
  </style>
  <link rel="icon" type="image/png" href="files/sbu.gif">
  <script async="" src="files/analytics.js"></script><script type="text/javascript" src="files/hidebib.js"></script>
  <title>Pranjal Sahu</title>
  <meta name="Pranjal Sahu&#39;s Stony Brook Homepage" http-equiv="Content-Type" content="Pranjal Sahu&#39;s Stony Brook Homepage">
  <link href="files/css" rel="stylesheet" type="text/css">
  
  <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-128589503-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'UA-128589503-1');
</script>


</head>

<body data-gr-c-s-loaded="true">
<table width="1000" border="0" align="center" cellspacing="0" cellpadding="20">
  <tbody><tr><td>

<p align="center"><font size="7">Pranjal Sahu</font><br>
    <b>Email</b>:
    <font id="email" style="display:inline;">psahu@cs.stonybrook.edu </font>
<br></br>
  <!-- </p><table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"> -->
  

  <tbody><tr>
    <td width="60%" valign="middle" align="justify">
    <p>I did my PhD in Computer Science under the guidance of <a href="http://www3.cs.stonybrook.edu/~qin/">Dr. Hong Qin</a> at Stony Brook University. My research focus 
      is on Deep Learning applications in the field of Biomedical Imaging. Some of the problems which I work on include image classification, projection de-noising, volume reconstruction etc. I did my summer internship at <a href="https://www.siemens-healthineers.com/en-us/">Siemens Healthineers</a>, Malvern in 2019 and 2020 where I worked on pathological lung volume segmentation from CT scans.
    <br/>    <br/>
    I did my Bachelors (Hons) in Computer Science from <a href="http://cse.iitkgp.ac.in/">IIT Kharagpur</a> in 2013 and later gained experience in software development working in startups for 3 years.
    I was fortunate to take Computer Vision and Computer Graphics courses under <a href="http://www.iitkgp.ac.in/department/EE/faculty/ee-rajiv">Dr. Rajiv Ranjan Sahay</a> and <a href="http://www.facweb.iitkgp.ac.in/~jay/">Dr. Jayanta Mukhopadhyay</a>  at IIT Kharagpur which motivated me to pursue PhD in this field. 
    I am active on Twitter where I regularly share interesting blog posts and resources related to Deep Learning, Computer Vision, Computer Graphics and Optimization.
    <br/> <br/>
     <h1 style="font-size:1.05vw">I defended my PhD thesis in May 2021 and will be joining Kitware as Senior R&D Engineer in Carrboro, NC. </h1>
    <!--Any student planning to apply for graduate studies in USA,  can contact me for any queries. I would be happy to help! -->
    </a><p align="center"><a>
    <a href="allpdfs/CV.pdf"> CV </a> |
    </a><a href="https://scholar.google.com/citations?user=W0LtyTkAAAAJ&hl=en&amp;hl=en&amp;oi=ao">Google Scholar</a> | <a href="https://github.com/PranjalSahu">Github</a> | <a href="https://twitter.com/pranjalsahu"> Twitter </a> | 
    <a href="https://www.linkedin.com/in/pranjal-sahu-89a3a623"> LinkedIn </a> |
    <a href="https://www.researchgate.net/profile/Pranjal_Sahu3"> ResearchGate </a> |
    <a href="https://www.proquest.com/openview/c52b41bc37a2716120d083aaa6090e69/1.pdf?pq-origsite=gscholar&cbl=18750&diss=y"> PhD Thesis </a>
    
    </p>
    </td>

    <td width="30%"><img src="files/profile_pranjal.png" width="95%"></a></td>
  </tr>
</tbody></table>

<table width="60%" align="center" border="0" cellspacing="0" cellpadding="20">
  <tbody><tr><td><sectionheading>Journal Publications</sectionheading></td></tr>
</tbody></table>
<table width="60%" align="center" border="0" cellspacing="0" cellpadding="20">

  <tbody>




<tr>
    <td width="33%" valign="top" align="center"><img src="files/intror.png" alt="sym" width="90%" style="border-style: none"></a>
    </td><td width="67%" valign="top">
      <p><a id="JBHI20">
      <heading>Structure Correction for Robust Volume Segmentation in Presence of Tumors</heading></a>
      <br><strong>Pranjal Sahu</strong>, Yiyuan Zhao, Parmeet Bhatia, Luca Bogoni, Anna Jerebko, and Hong Qin<br>
      </p>

      <div class="paper" id="lightweightnodule2020">
      <a href="javascript:toggleblock(&#39;largeScaleCuriosity2020_abs&#39;)">abstract</a> |
      <a href="https://www.researchgate.net/publication/342368058_Structure_Correction_for_Robust_Volume_Segmentation_in_Presence_of_Tumors">pdf</a> |
      <!--<a shape="rect" href="javascript:togglebib(&#39;lightweightnodule2018&#39;)" class="togglebib">bibtex</a> -->
      <a shape="rect" href="javascript:togglebib(&#39;lightweightnodule2020&#39;)" class="togglebib">bibtex</a>
      <em><br>IEEE Journal of Biomedical and Health Informatics, J-BHI</em>, 2020<br> <b>Impact Factor: 5.180</b>

      <p align="justify"> <i id="largeScaleCuriosity2020_abs" style="display: none;">CNN based lung segmentation models in absence of diverse training dataset fail to segment lung volumes in presence of severe pathologies such as large masses, scars, and tumors. To rectify this problem, we propose a multi-stage algorithm for lung volume segmentation from CT scans. The algorithm uses a 3D CNN in the first stage to obtain a coarse segmentation of the left and right lungs. In the second stage, shape correction is performed on the segmentation mask using a 3D structure correction CNN. A novel data augmentation strategy is adopted to train a 3D CNN which helps in incorporating global shape prior. Finally, the shape corrected segmentation mask is up-sampled and refined using a parallel flood-fill operation. The proposed multi-stage algorithm is robust in the presence of large nodules/tumors and does not require labeled segmentation masks for entire pathological lung volume for training. Through extensive experiments conducted on publicly available datasets such as NSCLC, LUNA, and LOLA11 we demonstrate that the proposed approach improves the recall of large juxtapleural tumor voxels by at least 15% over state-of-the-art models without sacrificing segmentation accuracy in case of normal lungs. The proposed method also meets the requirement of CAD software by performing segmentation within 5 seconds which is significantly faster than present methods..</i></p>

<pre xml:space="preserve" style="display: none;">@article{9122557,
  author={P. {Sahu} and Y. {Zhao} and P. {Bhatia} 
  and L. {Bogoni} and A. {Jerebko} and H. {Qin}},
  journal={IEEE Journal of Biomedical and Health Informatics}, 
  title={Structure Correction for Robust Volume 
  Segmentation in Presence of Tumors}, 
  year={2020},
  volume={},
  number={},
  pages={1-1},
  doi={10.1109/JBHI.2020.3004296}}
</pre>

      </div>
    </td>
  </tr>

<tr>
    <td width="33%" valign="top" align="center"><img src="files/lightweightlung.png" alt="sym" width="90%" style="border-style: none"></a>
    </td><td width="67%" valign="top">
      <p><a id="JBHI18">
      <!-- <img src="files/new.png" alt="[NEW]" width="6%" style="border-style: none"> -->
      <heading>A Lightweight Multi-section CNN for Lung Nodule Classification and Malignancy Estimation</heading></a>
      <br><strong>Pranjal Sahu</strong>, Dantong Yu, Mallesham Dasari, Fei Hou and Hong Qin<br>
      </p>

      <div class="paper" id="lightweightnodule2018">
      <a href="javascript:toggleblock(&#39;largeScaleCuriosity2018_abs&#39;)">abstract</a> |
      <a href="https://www.researchgate.net/publication/328774789_A_Lightweight_Multi-section_CNN_for_Lung_Nodule_Classification_and_Malignancy_Estimation">pdf</a> |
      <!--<a shape="rect" href="javascript:togglebib(&#39;lightweightnodule2018&#39;)" class="togglebib">bibtex</a> -->
      <a shape="rect" href="javascript:togglebib(&#39;lightweightnodule2018&#39;)" class="togglebib">bibtex</a>
      <em><br>IEEE Journal of Biomedical and Health Informatics, J-BHI</em>, 2018<br> <b>Impact Factor: 5.180 </b>

      <p align="justify"> <i id="largeScaleCuriosity2018_abs" style="display: none;">The size and shape of a nodule are the essential
indicators of malignancy in lung cancer diagnosis.
However, effectively capturing the nodule\92s structural
information from CT scans in a Computer-aided system
is a challenging task. Unlike previous models which proposed
computationally intensive deep ensemble models
or 3D CNN models, we propose a lightweight, multiple
view sampling based Multi-section CNN architecture. The
model obtains a nodule\92s cross-sections from multiple view
angles and encodes the nodule\92s volumetric information
into a compact representation by aggregating information
from its different cross-sections via a view pooling layer.
The compact feature is subsequently used for the task of
nodule classification. The method does not require nodule\92s
spatial annotation and works directly on the crosssections
generated from volume enclosing the nodule. We
evaluated the proposed method on LIDC-IDRI dataset. It
achieved state-of-the-art performance with a mean 93.18%
classification accuracy. The architecture could also be used
to select the representative cross-sections determining
nodule\92s malignancy which facilitates in the interpretation
of results. Because of being lightweight the model could
be ported to mobile devices which brings the power of AI
driven application directly into practitioner\92s hand.</i></p>

<pre xml:space="preserve" style="display: none;">@article{sahu2018lightweight,
  title={A lightweight multi-section CNN for lung nodule 
  classification and malignancy estimation},
  author={Sahu, Pranjal and Yu, Dantong and Dasari, 
  Mallesham and Hou, Fei and Qin, Hong},
  journal={IEEE journal of biomedical and health informatics},
  volume={23},
  number={3},
  pages={960--968},
  year={2018},
  publisher={IEEE}
}
</pre>

      </div>
    </td>
  </tr>
  
<table width="60%" align="center" border="0" cellspacing="0" cellpadding="20">
  <tbody><tr><td><sectionheading>Conference Publications</sectionheading></td></tr>
</tbody></table>
<table width="60%" align="center" border="0" cellspacing="0" cellpadding="20">

<tr>
    <td width="33%" valign="top" align="center"><img src="files/ipmi_supp_temp1.png" alt="sym" width="90%" style="border-style: none"></a>
    </td><td width="67%" valign="top">
      <p><a id="MICCAI21">
      <img src="files/new.png" alt="[NEW]" width="6%" style="border-style: none">
      <heading>Interactive Smoothing Parameter Optimization in DBT Reconstruction using Deep learning</heading></a>
      <br><strong>Pranjal Sahu</strong>, Hailiang Huang, Wei Zhao, and Hong Qin<br>
      </p>

      <div class="paper" id="miccai_abs2021">
      <a href="javascript:toggleblock(&#39;MICCAI21_abs&#39;)">abstract</a> |
      <a href="https://www.researchgate.net/publication/354787910_Interactive_Smoothing_Parameter_Optimization_in_DBT_Reconstruction_Using_Deep_Learning">pdf</a> |
      <a shape="rect" href="javascript:togglebib(&#39;miccai_abs2021&#39;)" class="togglebib">bibtex</a>
      <em><br>Medical Image Computing and Computer Assisted Intervention, MICCAI</em>, 2021<br>
      <p align="justify"> <i id="MICCAI21_abs" style="display: none;">Medical image reconstruction algorithms such as PenalizedWeighted Least Squares (PWLS) 
        typically rely on a good choice of tuningparameters such as the number of iterations, the strength of regularizar,etc. 
        However, obtaining a good estimate of such parameters is often doneusing trial and error methods. 
        This process is very time consuming andlaborious  especially  for  high  resolution  images.  
        To  solve  this  problemwe  propose  an  interactive  framework.  We  focus  on  the  regularizationparameter and train a CNN to imitate its 
        impact on image for varyingvalues. The trained CNN can be used by a human practitioner to tunethe regularization strength on-the-fly as per the requirements. 
        Taking theexample of Digital Breast Tomosynthesis reconstruction, we demonstrate the feasibility of our approach and 
        also discuss the future applications of  this  interactive  reconstruction  approach.  We  also  test  the  proposed methodology 
        on public Walnut and Lodopab CT reconstruction datasetsto show it can be generalized to CT reconstruction as well.</i></p>

<pre xml:space="preserve" style="display: none;">@inproceedings{sahu2021interactive,
  title={Interactive Smoothing Parameter Optimization in DBT Reconstruction Using Deep Learning},
  author={Sahu, Pranjal and Huang, Hailiang and Zhao, Wei and Qin, Hong},
  booktitle={International Conference on Medical Image Computing and Computer-Assisted Intervention},
  pages={57--67},
  year={2021},
  organization={Springer}
}

      </div>
    </td>
  </tr>

<tr>
    <td width="33%" valign="top" align="center"><img src="files/infocom.png" alt="sym" width="100%" style="border-style: none"></a>
    </td><td width="67%" valign="top">
      <p><a id="rsna19">
      <heading>Streaming 360-Degree Videos Using Super-Resolution</heading>
      </a>      <br>Mallesham Dasari, Arani Bhattacharya, Santiago Vargas, <strong>Pranjal Sahu</strong>, et al.<br>
      </p>


<div class="paper" id="infocom2020">
      <a href="javascript:toggleblock(&#39;infocom2020a&#39;)">abstract</a> |
      <a href="https://www.researchgate.net/publication/339114778_Streaming_360-Degree_Videos_Using_Super-Resolution">pdf</a> |
      <a shape="rect" href="javascript:togglebib(&#39;infocom2020&#39;)" class="togglebib">bibtex</a>
      <br><em>INFOCOM,</em> 2020 <br>

      <p align="justify"> <i id="infocom2020a" style="display: none;">360 degree videos provide an immersive experience to users, but require considerably more bandwidth to stream compared to regular videos. State-of-the-art 360 \BF video streaming systems use viewport prediction to reduce bandwidth requirement , that involves predicting which part of the video the user will view and only fetching that content. However, viewport prediction is error prone resulting in poor user Quality of Experience (QoE). We design PARSEC, a 360 \BF video streaming system that reduces bandwidth requirement while improving video quality. PARSEC trades off bandwidth for additional client-side computation to achieve its goals. PARSEC uses an approach based on super-resolution, where the video is significantly compressed at the server and the client runs a deep learning model to enhance the video to a much higher quality. PARSEC addresses a set of challenges associated with using super-resolution for 360 \BF video streaming: large deep learning models, slow inference rate, and variance in the quality of the enhanced videos. To this end, PAR-SEC trains small micro-models over shorter video segments, and then combines traditional video encoding with super-resolution techniques to overcome the challenges. We evaluate PARSEC on a real WiFi network, over a broadband network trace released by FCC, and over a 4G/LTE network trace. PARSEC significantly outperforms the state-of-art 360 \BF video streaming systems while reducing the bandwidth requirement.</i></p>
<pre xml:space="preserve" style="display: none;">
@inproceedings{dasari2020streaming,
  title={Streaming 360-degree videos using super-resolution},
  author={Dasari, Mallesham and Bhattacharya, Arani and Vargas, Santiago and Sahu, Pranjal and 
  Balasubramanian, Aruna and Das, Samir R},
  booktitle={IEEE INFOCOM 2020-IEEE Conference on Computer Communications},
  pages={1977--1986},
  year={2020},
  organization={IEEE}
}
</div>


    </td>
  </tr>


<tr>
    <td width="33%" valign="top" align="center"><img src="files/rsna1.jpg" alt="sym" width="100%" style="border-style: none"></a>
    </td><td width="67%" valign="top">
      <p><a id="rsna19">
      <heading>Scatter correction with deep learning approach for contrast enhanced digital breast tomosynthesis (CEDBT) 
        in both cranio-caudal (CC) view and mediolateral oblique (MLO) view</heading>
      </a>      <br>Xiaoyu Duan, <strong>Pranjal Sahu</strong>, Hailiang Huang, Wei Zhao<br>
      </p>


<div class="paper" id="iwbi2020a">
      <a href="javascript:toggleblock(&#39;iwbi_2020a&#39;)">abstract</a> |
      <a href="https://www.researchgate.net/publication/341590128_Scatter_correction_with_deep_learning_approach_for_contrast_enhanced_digital_breast_tomosynthesis_CEDBT_in_both_cranio-caudal_CC_view_and_mediolateral_oblique_MLO_view">pdf</a> |
      <a shape="rect" href="javascript:togglebib(&#39;iwbi2020a&#39;)" class="togglebib">bibtex</a>
      <br><em>IWBI,</em> 2020 <strong>(Oral)</strong> <br>

      <p align="justify"> <i id="iwbi_2020a" style="display: none;">Contrast enhanced digital breast tomosynthesis (CEDBT) utilizes weighted subtraction of high energy (HE) and low energy (LE) DBT
to generate a 3D iodinated contrast enhancement map of the breast, and potentially improve breast lesion detection and
characterization. However, the increased scattered radiation at HE exacerbates the cupping artifact. Monte Carlo (MC) based
scatter correction (SC) method suffers from long computation time, and kernel-based method is less accurate, especially near the
breast edge due to thickness roll-off. This work is aimed at developing fast and accurate SC using Convolutional Neural Network
(CNN).</i></p>
<pre xml:space="preserve" style="display: none;">
@inproceedings{duan2020scatter,
  title={Scatter correction with deep learning
  approach for contrast enhanced digital
  breast tomosynthesis (CEDBT) in both
  cranio-caudal (CC) view and mediolateral oblique (MLO) view},
  author={Duan, Xiaoyu and Sahu, Pranjal and
  Huang, Hailiang and Zhao, Wei},
  booktitle={15th International Workshop
  on Breast Imaging (IWBI2020)},
  volume={11513},
  pages={115130Q},
  year={2020},
  organization={International Society
  for Optics and Photonics}
}
</div>


    </td>
  </tr>

<!-- <tr>
    <td width="33%" valign="top" align="center"><img src="files/rsna1.jpg" alt="sym" width="100%" style="border-style: none"></a>
    </td><td width="67%" valign="top">
      <p><a id="rsna19">
      <img src="files/new.png" alt="[NEW]" width="6%" style="border-style: none">
      <heading>Effect of scatter correction on image noise in contrast-enhanced digital breast tomosynthesis</heading>
      </a>      <br>Hailiang Huang, Xiaoyu Duan, <strong>Pranjal Sahu</strong>, Wei Zhao<br>
      </p>


<div class="paper" id="iwbi2020b">
      <a href="javascript:toggleblock(&#39;iwbi_2020b&#39;)">abstract</a> |
      <a href="https://www.researchgate.net/publication/341590827_Effect_of_scatter_correction_on_image_noise_in_contrast-enhanced_digital_breast_tomosynthesis">pdf</a> |
      <a shape="rect" href="javascript:togglebib(&#39;iwbi2020b&#39;)" class="togglebib">bibtex</a>
      <br><em>IWBI,</em> 2020  <br>

      <p align="justify"> <i id="iwbi_2020b" style="display: none;">Abstract</i></p>
<pre xml:space="preserve" style="display: none;">
@inproceedings{huang2020effect,
  title={Effect of scatter correction on image noise 
  in contrast-enhanced digital breast tomosynthesis},
  author={Huang, Hailiang and Duan, Xiaoyu and 
  Sahu, Pranjal and Zhao, Wei},
  booktitle={15th International Workshop 
  on Breast Imaging (IWBI2020)},
  volume={11513},
  pages={115130J},
  year={2020},
  organization={International Society 
  for Optics and Photonics}}     
</div> -->

      
    </td>
  </tr>


<tr>
    <td width="33%" valign="top" align="center"><img src="files/openvct_pipeline.PNG" alt="sym" width="100%" style="border-style: none"></a>
    </td><td width="67%" valign="top">
      <p><a id="ISBI19">
      <heading>Using virtual digital breast tomosynthesis for de-noising of low-dose projection images</heading></a>
      <br><strong>Pranjal Sahu</strong>, Hailiang Huang, Wei Zhao and Hong Qin<br>
      </p>

      <div class="paper" id="isbi2019dbt">
      <a href="javascript:toggleblock(&#39;dbt_2018&#39;)">abstract</a> |
      <a href="https://www.researchgate.net/publication/334418641_Using_Virtual_Digital_Breast_Tomosynthesis_for_De-Noising_of_Low-Dose_Projection_Images">pdf</a> |
      <a shape="rect" href="javascript:togglebib(&#39;isbi2019dbt&#39;)" class="togglebib">bibtex</a>
      <br><em>IEEE International Symposium on Biomedical Imaging, ISBI,</em> 2019<br>

      <p align="justify"> <i id="dbt_2018" style="display: none;">Digital Breast Tomosynthesis (DBT) provides a 
        quasi-3D impression of the breast volume resulting in a better visualization
of mass. However, one serious drawback of Tomosynthesis
is that compared to Mammography, each projection gets
lower x-ray dose resulting into higher quantum noise which
seriously hampers the visibility of calcifications. To solve this
problem we propose a Convolutional Neural Network model
based on Adversarial loss. We train the deep network using
synthetic data obtained from Virtual Clinical Trials. Unlike
earlier works which tested model on phantoms only, we performed
experiments on real samples obtained in clinical settings
as well. Our approach shows encouraging results in denoising
the projections. De-noised projections show higher
perceptual similarity with mammograms and superior signalto-
noise ratio. The reconstructed volume also enhances calcification
visibility. Our work shows the viability of utilizing
synthetic data for training the deep network for de-noising
purposes.</i></p>
<pre xml:space="preserve" style="display: none;">
@inproceedings{sahu2019using,
  title={Using Virtual Digital Breast Tomosynthesis 
  for De-Noising of Low-Dose Projection Images},
  author={Sahu, Pranjal and Huang, Hailiang and 
  Zhao, Wei and Qin, Hong},
  booktitle={2019 IEEE 16th International Symposium on 
  Biomedical Imaging (ISBI 2019)},
  pages={1647--1651},
  year={2019},
  organization={IEEE}
}
</pre>
      </div>
    </td>
  </tr>


  <tr>
    <td width="33%" valign="top" align="center"><a ><img src="files/spie_image.jpg" alt="sym" width="100%" style="border-style: none"></a>
    </td><td width="67%" valign="top">
      <p><a id="SPIE18">
      <heading>Apply lightweight deep learning on internet of things for low-cost and easy-to-access skin cancer detection</heading></a><br>
      <strong>Pranjal Sahu</strong>, Dantong Yu, Hong Qin<br>
      </p>

      <div class="paper" id="cvprw18">
      <a href="javascript:toggleblock(&#39;cvprw18_abs&#39;)">abstract</a> |
      <a href="https://www.researchgate.net/publication/323590913_Apply_lightweight_deep_learning_on_internet_of_things_for_low-cost_and_easy-to-access_skin_cancer_detection">pdf</a> |
      <a shape="rect" href="javascript:togglebib(&#39;cvprw18&#39;)" class="togglebib">bibtex</a>
      <br><em>Medical Imaging, SPIE, 2018 </em><strong>(Best Demo Award)</strong><br>

      <p align="justify"> <i id="cvprw18_abs" style="display: none;">Melanoma is the most dangerous form of skin cancer that often resembles moles. Dermatologists often rec-ommend regular skin examination to identify and eliminate Melanoma in its early stages. To facilitate thisprocess, we propose a hand-held computer (smart-phone, Raspberry Pi) based assistant that classifies with thedermatologist-level accuracy skin lesion images into malignant and benign and works in a standalone mobiledevice without requiring network connectivity. In this paper, we propose and implement a hybrid approachbased on advanced deep learning model and domain-specific knowledge and features that dermatologists usefor the inspection purpose to improve the accuracy of classification between benign and malignant skin lesions.Here, domain-specific features include the texture of the lesion boundary, the symmetry of the mole, and theboundary characteristics of the region of interest. We also obtain standard deep features from a pre-trainednetwork optimized for mobile devices called Google\92s MobileNet. The experiments conducted on ISIC 2017 skincancer classification challenge demonstrate the effectiveness and complementary nature of these hybrid featuresover the standard deep features. We performed experiments with the training, testing and validation data splitsprovided in the competition. Our method achieved area of 0.805 under the receiver operating characteristiccurve. Our ultimate goal is to extend the trained model in a commercial hand-held mobile and sensor devicesuch as Raspberry Pi and democratize the access to preventive health care.</i></p>

<pre xml:space="preserve" style="display: none;">
  @inproceedings{sahu2018apply,
  title={Apply lightweight deep learning 
  on internet 
  of things for low-cost and 
  easy-to-access 
  skin cancer detection},
  author={Sahu, Pranjal and Yu, 
  Dantong and Qin, Hong},
  booktitle={Medical Imaging 2018: 
  Imaging Informatics 
  for Healthcare, Research, and Applications},
  volume={10579},
  pages={1057912},
  year={2018},
  organization={International Society 
  for Optics and Photonics}
}
</pre>
      </div>
    </td>
  </tr>

  <!-- <tr>
    <td width="33%" valign="top" align="center"><a ><img src="files/hpdc.jpg" alt="sym" width="100%" style="border-style: none"></a>
    </td><td width="67%" valign="top">
      <p><a id="HPDC18">
      <heading>In-Operando Tracking and Prediction of Transition in Material System using LSTM</heading></a><br><strong>Pranjal Sahu</strong>, Dantong Yu, Kevin Yager, Mallesham Dasari and Hong Qin<br>
      
      </p>

      <div class="paper" id="compgan18">
      <a href="javascript:toggleblock(&#39;compgan18_abs&#39;)">abstract</a> |
      <a href="https://www.researchgate.net/publication/325639886_In-Operando_Tracking_and_Prediction_of_Transition_in_Material_System_using_LSTM">pdf</a> |
      <a shape="rect" href="javascript:togglebib(&#39;compgan18&#39;)" class="togglebib">bibtex</a>
      <br>
      <em>Autonomous Infrastructure for Science, HPDC</em>, 2018<br>
      <p align="justify"> <i id="compgan18_abs" style="display: none;">The structures of many material systems evolve as they are
treated with physical processing. For instance, organic and
inorganic crystalline materials frequently coarsen over time
as they are thermally treated; with domains (grains) rotating
and growing in size. When a material system undergoing the
structural transformation is probed using x-ray scattering
beams, the peaks in the scattering images will sharpen and
intensify, and the scattering rings will become increasingly
\92textured\92. Accurate identification of the transition frame in
advance brings multiple benefits to the NSLS-II in-operando
experiments of studying material systems such as minimal
beamline damage to samples, reduced energy costs, and the
optimal sampling of material properties. In this paper, we
formulate the prediction and identification of the structural
transition event as a classification problem and apply a novel
LSTM model to identify sequences having transition event.
The preliminary results of the experiments are encouraging
and confirm the viability of the detection and prediction of
transition in advance. Our ultimate goal is to deploy such a
prediction system in the real-world environment at the selected
beamline of NSLS-II for improving the efficiency of
the experimental facility.</i></p>

<pre xml:space="preserve" style="display: none;">@inproceedings{sahu2018operando,
  title={In-Operando Tracking and Prediction of 
  Transition in Material System using LSTM},
  author={Sahu, Pranjal and Yu, Dantong and 
  Yager, Kevin and Dasari, Mallesham 
  and Qin, Hong},
  booktitle={Proceedings of the 1st 
  International 
  Workshop on Autonomous 
  Infrastructure for Science},
  pages={6},
  year={2018},
  organization={ACM}
}
</pre>
      </div>
    </td>
  </tr> -->

  <!-- <tr>
    <td width="33%" valign="top" align="center"><a ><img src="files/protein.png" alt="sym" width="80%" border="1" style="border-color:black"></a>
    </td><td width="67%" valign="top">
      <p><a id="SHREC17">
      <heading>SHREC'17 Track: Protein Shape Retrieval</heading></a><br>
      Na Song, Daniela Craciun and others<br>
      
      </p>

      <div class="paper" id="iclr18">
      <a href="javascript:toggleblock(&#39;iclr18_abs&#39;)">abstract</a> |
      <a href="https://www.researchgate.net/publication/327836044_SHREC'17_Track_Protein_Shape_Retrieval">pdf</a> |
      <a shape="rect" href="javascript:togglebib(&#39;iclr18&#39;)" class="togglebib">bibtex</a> 
      <br>
      <em>Eurographics Workshop on 3D Object Retrieval</em>, 2017<br>
      <p align="justify"> <i id="iclr18_abs" style="display: none;">The large number of protein structures deposited in the protein database provide an opportunity to examine the structure relations using computational algorithms, which can be used to classify the structures based on shape similarity. In this paper, we report the result of the SHREC 2017 track on shape retrievals from protein database. The goal of this track is to test the performance of the algorithms proposed by participants for the retrieval of bioshape (proteins). The test set is composed of 5,854 abstracted shapes from actual protein structures after removing model redundancy. Ten query shapes were selected from a set of representative molecules that have important biological functions. Six methods from four teams were evaluated and the performance is summarized in this report, in which both the retrieval accuracy and computational speed were compared. The biological relevance of the shape retrieval approaches is discussed. We also discussed the future perspectives of shape retrieval for biological molecular models.</i></p> -->



<pre xml:space="preserve" style="display: none;">@inproceedings{Song2017SHREC17TP,
  title={SHREC\9217 Track: Protein Shape Retrieval},
  author={Na Song and Daniela Craciun and 
  Charles Christoffer and Xusi Han and 
  Daisuke Kihara and Guillaume Levieux 
  and Matthieu Montes and Hong Qin and 
  Pranjal Sahu and Genki Terashi and 
  Haiguang Liu},
  year={2017}
}</pre>
      </div>
    </td>
  </tr>
</tbody></table>

<table width="60%" align="center" border="0" cellpadding="20">
  <tbody><tr><td><sectionheading>Teaching</sectionheading></td></tr>
  <tr>
    <td width="33%" align="center"><img src="files/medical.jpg" alt="pacman" width="80%"></td>
    <td width="67%" valign="center">
      <p>
        <a href="http://www3.cs.stonybrook.edu/~cse328/index.html"><heading>CSE 328: Fundamentals of Computer Graphics </heading></a><br>
        <strong>Instructor</strong>: Dr. Hong Qin<br>
      </p>
      <p>
        <a ><heading>CSE 377: Medical Imaging</heading></a><br>
        <strong>Instructor</strong>: Dr. Allen Tannenbaum<br>
      </p>
    </td>
  </tr>
</tbody></table>

<table width="60%" align="center" border="0" cellpadding="20">
  <tbody><tr><td>
    <sectionheading>Awards and Talks</sectionheading>
    <ul> 
    <li> Invited to give talk at Bell labs, Murray Hill on Deep Learning applications in Medical Imaging (2019)</li>
    <li> Computer Science Chairman Fellowship (2016-2017)</li>
    <li> Best Demo Award in SPIE Medical Imaging, Houston (2018)</li>
    <li> Represented (C.G.) state in National Children Science Congress, Guwahati (2005)</li>
    </ul>
  </td></tr>

</tbody></table>

<table width="60%" align="center" border="0" cellpadding="20">
<tbody><tr><td>
<a font-size="2" class="twitter-timeline" data-width="800" data-height="400" href="https://twitter.com/pranjalsahu?ref_src=twsrc%5Etfw">Tweets by pranjalsahu </a> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
</td></tr>
</tbody></table>

  </td></tr>
</tbody></table>
<script xml:space="preserve" language="JavaScript">
hideallbibs();
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('jpm15_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('fg15_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('wacv15_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('iclr15_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('cvpr15_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('iccv15_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('jmlr16_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('cvpr16_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('iclr16_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('cvpr17_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('icml17_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('nips17_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('iclr18_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('iclrw18_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('cvprw18_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('largeScaleCuriosity2018_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('compgan18_abs');
</script>



</body></html>
